##########################################################################################
#
#   Local EMBL-EBI Training Team VMware Horizon VMs deployment script.
#   Runs on HPE Synergy 480 Gen10 with NVidia Tesla M10 GPUs
#   This playbook deployes VMs to externally accessible
#   last updated on 22-02-2021
#
##########################################################################################
- name: Clone linked VMware VMs on External VLAN. Last update on 09-06-2021
  hosts: localhost
  gather_facts: yes
  vars:
    cluster: HL-VDI
    ###### HL-VDI Top-Chassis Bottom-Chassis
    #datastore: flash-ds1
    ###### slower-bigger flash-ds1
    #vlanname: "HL Training External"
    ###### "HL Training Internal" "HL Training External"
    #vmsnapshot: Post_Nvidia

##########################################################################
    #coursevm: introrna-seqvm
    #sourcevm: IntroRNASeqMar21
    #############################################
    #nbr: 40    # Number of VMs to be deployed
    #strt: 1   # index of first VM
    #############################################
    #Access_users_pasword: TungstenTrebleAmplifier21
    #Access_trainers_pasword: BriskElseMonument21
    #############################################
    #GPU_Profile: "grid_m10-8q"
    ###### grid_m10-8q grid_m10-4q
##########################################################################

  tasks:
    - name: Make sure the master VM is poweroff
      community.vmware.vmware_guest_powerstate:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        validate_certs: no
        name: "{{ sourcevm }}"
        state: powered-off

    - name: Migrate the master VM to a host with GPU attached
      community.vmware.vmware_vmotion:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        validate_certs: no
        vm_name: "{{ sourcevm }}"
        destination_host: "{{ first_gpu_host }}"

    - name: Adding vGPU profile to the master VM
      community.vmware.vmware_guest_vgpu:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        datacenter: "{{ datacenter }}"
        validate_certs: no
        name: "{{ sourcevm }}"
        vgpu: "{{ GPU_Profile }}"
        state: present
      register: pci

    - name: Make a snapshot to ensure the vGPU profile attached
      community.vmware.vmware_guest_snapshot:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        datacenter: "{{ datacenter }}"
        validate_certs: no
        folder: "{{ vmfolder }}"
        name: "{{ sourcevm }}"
        state: present
        snapshot_name: "{{ vmsnapshot }}"
        description: "Snapshot after attaching a GPU profile"
      register: vmsnap

    - name: Clones the VMs
      community.vmware.vmware_guest:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        validate_certs: no
        datacenter: "{{ datacenter }}"
        cluster: "{{ cluster }}"
        linked_clone: yes
        snapshot_src: "{{ vmsnapshot }}"
        template: "{{ sourcevm }}"
        folder: "{{ vmfolder }}"
        name: "{{ coursevm }}-{{ item }}"
        state: poweredon
        wait_for_ip_address: yes
        #state: present
        datastore: "{{ datastore }}"
        networks:
          - name: "{{ vlanname }}"
            start_connected: yes
            type: dhcp
            domain: "{{ vmdomain }}"
        #resource_pool: "{{ cluster }}"
        customization:
          domain : "{{ vmdomain }}"
          hostname: "{{ coursevm }}-{{ item }}"
          timezone: "Europe/London"
          dns_servers: "{{ vmdnsserver }}"
      with_sequence: start={{strt}} end={{nbr}} format=%02d
      #stride=2
      delegate_to: localhost
      async: 120
      poll: 0

    - name: Wait for last VM to be ready for 2 minutes
      wait_for:
        timeout: 120
      delegate_to: localhost
      #when: vmsnap.changed

    - name: Make sure the VMs are powered on
      community.vmware.vmware_guest:
        hostname: "{{ vcenter_hostname }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        validate_certs: no
        datacenter: "{{ datacenter }}"
        cluster: "{{ cluster }}"
        folder: "{{ vmfolder }}"
        name: "{{ coursevm }}-{{ item }}"
        state: poweredon
      with_sequence: start={{strt}} end={{nbr}} format=%02d
      delegate_to: localhost
      register: clonedvm

    - debug:
        #var: clonedvm | json_query("results[*].instance.ipv4")
        var: clonedvm
        
    #- name: Wait for 2nd time for all VMs to be have IPs
    #  wait_for:
    #    timeout: 60
    #  delegate_to: localhost
    #  with_items: "{{ clonedvm | json_query('results[*].instance.ipv4') }}"
    #  when: item == 'null'
    #  register: secondrestart

    #- name: Make sure the VMs are powered on and with IPs
    #  community.vmware.vmware_guest:
    #    hostname: "{{ vcenter_hostname }}"
    #    username: "{{ vcenter_username }}"
    #    password: "{{ vcenter_password }}"
    #    validate_certs: no
    #    datacenter: "{{ datacenter }}"
    #    cluster: "{{ cluster }}"
    #    folder: "{{ vmfolder }}"
    #    name: "{{ coursevm }}-{{ item }}"
    #    state: poweredon
    #  with_sequence: start={{strt}} end={{nbr}} format=%02d
    #  delegate_to: localhost
    #  register: clonedvm
     
    #- name: Wait for 3rd time for all VMs to be have IPs
    #  wait_for:
    #    timeout: 60
    #  delegate_to: localhost
    #  with_items: "{{ clonedvm | json_query('results[*].instance.ipv4') }}"
    #  when: item == 'null'
    #  register: secondrestart
     
    - name: Add the newly created VMs to the playbook inventory
      add_host:
        name: "{{ item.0 }}"
        instance_name: "{{ item.0 }}"
        ansible_host: "{{ item.1 }}"
        groups: float
        brokerip: "{{ brokerip }}"
        vmdomain: "{{ vmdomain }}"
        brokeruser: "{{ brokeruser }}"
        brokerpass: "{{ brokerpass }}"
      with_together:
        - "{{ clonedvm | json_query('results[*].instance.hw_name') }}"
        - "{{ clonedvm | json_query('results[*].instance.ipv4') }}"

    - name: Remove old VMs pool file from inventory
      file:
        path: "/ansible/ansible_playbooks/hosts.d/{{ coursevm }}"
        state: absent
      ignore_errors: yes

    - name: Fill in VMs pool inventory file
      blockinfile:
        path: "/ansible/ansible_playbooks/hosts.d/{{ coursevm }}"
        create: yes
        block: |
          [{{ coursevm }}:vars]
          remote_user=setup
          brokerip="{{ brokerip }}"
          vmdomain="{{ vmdomain }}"
          brokeruser="{{ brokeruser }}"
          brokerpass="{{ brokerpass }}"

          [{{ coursevm }}]

    - name: Fill in backup hosts file
      lineinfile:
        line: "{{ item.0 }}        ansible_host={{ item.1 }}"
        path: "/ansible/ansible_playbooks/hosts.d/{{ coursevm }}"
        insertafter: EOF
      with_together:
        - "{{ clonedvm | json_query('results[*].instance.hw_name') }}"
        - "{{ clonedvm | json_query('results[*].instance.ipv4') }}"

    - name: Wait for SSH to come up
      wait_for:
        host: "{{ item }}"
        port: 22
        state: started
        #search_regex: OpenSSH
        #delay: 5
      with_items:
        ###- "{{ groups.TopChassis }}"
        - "{{ clonedvm | json_query('results[*].instance.ipv4') }}"
      connection: local

    - name: Update first user password for the new course
      community.windows.win_domain_user:
        name: "extuser1"
        password: "{{ Access_users_pasword }}"
        state: present
        update_password: when_changed
        enabled: yes
      delegate_to: "{{ Windows_Domain }}"
      register: userpass

    - name: Update Acces Users passwords for the new course
      community.windows.win_domain_user:
        name: "{{ item }}"
        password: "{{ Access_users_pasword }}"
        state: present
        update_password: when_changed
        enabled: yes
      delegate_to: "{{ Windows_Domain }}"
      when: userpass.changed
      with_items:
        - extuser2
        - extuser3
        - extuser4
        - extuser5
        - extuser6
        - extuser7
        - extuser8
        - extuser9
        - extuser10
        - extuser11
        - extuser12
        - extuser13
        - extuser14
        - extuser15
        - extuser16
        - extuser17
        - extuser18
        - extuser19
        - extuser20
        - extuser21
        - extuser22
        - extuser23
        - extuser24
        - extuser25
        - extuser26
        - extuser27
        - extuser28
        - extuser29
        - extuser30
        - extuser31
        - extuser32

    - name: Update Acces Trainers Passwords for the new course
      community.windows.win_domain_user:
        name: "{{ item }}"
        password: "{{ Access_trainers_pasword }}"
        state: present
        update_password: when_changed
        enabled: yes
      delegate_to: "{{ Windows_Domain }}"
      with_items:
        - exttrainer1
        - exttrainer2
        - exttrainer3
        - exttrainer4
        - exttrainer5
        - exttrainer6
        - exttrainer7
        - exttrainer8
        - exttrainer9
        - exttrainer10
        #- exttrainer11
        #- exttrainer12
        #- exttrainer13
        #- exttrainer14
        #- exttrainer15
        #- exttrainer16
        #- exttrainer17
        #- exttrainer18
        #- exttrainer19
        #- exttrainer20

##########################################################################
##########################################################################
- hosts: float
  # TopCluster  float
  name: Configure VMs instances
  remote_user: setup
  become: True
  gather_facts: True
  tasks:
    - name: Install required packages for the NVIDIA driver installer
      apt:
        name: "{{ packages }}"
        update_cache: yes
      vars:
        packages:
        - xorg
        - expect
        - xterm
        - mesa-utils
        - default-jre
        - i3
        - libgl1-mesa-dri:i386
        - libgl1-mesa-glx:i386
        - pulseaudio
        - libglvnd-dev
        - libglvnd-core-dev
        - libglvnd0
        - pkg-config
        - build-essential
        - x11-xserver-utils
        - nfs-common
        - libpam-pkcs11
        - libnss3-tools
        - open-vm-tools
        - python
        - python-dbus
        - python-gobject

    - name: Configure local DNS to point to the NFS file server
      #lineinfile : path=/etc/hosts line='10.33.32.20   penelopeprime.training.ebi.ac.uk   penelopeprime'
      lineinfile : path=/etc/hosts regexp='^10.33.32.20*' line='10.33.36.9   penelopeprime.training.ebi.ac.uk   penelopeprime'

 #   - name: Remove old fileserver DNs configuration
 #     lineinfile : path=/etc/hosts regexp='^10.7.243.68'  line=' '

    - name: Unmount old NFS fileserver
      shell: umount /media/penelopeprime
      ignore_errors: yes

 #   - name: Remove old fileserver DNS configuration
 #     lineinfile : path=/etc/fstab regexp='^10.7.243.68'  line=' '

    - name: Make sure that the NFS configuration is well set
      #lineinfile : path=/etc/fstab line='10.33.32.20:/media/shared/Course_Materials /media/penelopeprime    nfs4    defaults     0 0'
      lineinfile : path=/etc/fstab regexp='^10.33.32.20:/media/shared/Course_Materials*' line='10.33.36.9:/media/shared/Course_Materials /media/penelopeprime    nfs4    defaults     0 0'

    - name: Make sure the NFS folder is there
      file: path=/media/penelopeprime state=directory mode=0777

    - name: Mount the NFS fileserver
      shell: mount -a

    - name: VM hostname configuration
      script: /ansible/ansible_playbooks/training_hoster {{inventory_hostname}}
      ignore_errors: yes

    - name: Configrue nvidia driver black list on modprob
      blockinfile:
        path: /etc/modprobe.d/blacklist.conf
        block: |
          blacklist nouveau
          blacklist lbm-nouveau
          options nouveau modeset=0

    - name: Add nouveau blacklist to grub
      lineinfile: dest=/etc/default/grub regexp='^GRUB_CMDLINE_LINUX=""' line='GRUB_CMDLINE_LINUX="rdblacklist=nouveau"'

    - name: Update Grub settings to apply new blacklist changes
      shell: sudo update-grub

    - name: Get the NVIDIA driver installer
      copy:
        #src: /media/penelopeprime/.Admin/Nvidia/NVIDIA-GRID-vSphere-6.7-460.73.02-460.73.01-462.31/NVIDIA-Linux-x86_64-460.73.01-grid.run
        src: /media/penelopeprime/.Admin/Nvidia/NVIDIA-GRID-vSphere-6.7-450.89-452.57/NVIDIA-Linux-x86_64-450.89-grid.run
        dest: /usr/local/NVIDIA-Linux-grid.run
        owner: setup
        mode: 0777
        remote_src: True

    - name: Stop X server to install nvidia driver
      shell: |
        systemctl stop lightdm
        systemctl stop gdm3

    - name: Configure Apport to stop messing around
      lineinfile:
        dest: /etc/default/apport
        regexp: '^enabled=1'
        line: enabled=0

    - name: Install NVIDIA GRID driver
      shell: /usr/local/NVIDIA-Linux-grid.run -q -a -n -X -s -Z
      # -Z
      ignore_errors: yes

    - name: Enable Nvidia Grid template file to be used for licence server config
      copy:
        src: /etc/nvidia/gridd.conf.template
        dest: /etc/nvidia/gridd.conf
        owner: root
        mode: 0611
        remote_src: True

    - name: Add NVIDIA GRID License server IP
      lineinfile: dest=/etc/nvidia/gridd.conf regexp='^ServerAddress=' line='ServerAddress=10.33.32.19'

    - name: Add NVIDIA GRID License server port
      lineinfile: dest=/etc/nvidia/gridd.conf regexp='^ServerPort=' line='ServerPort=7070'

    - name: Add NVIDIA GRID License feature
      lineinfile: dest=/etc/nvidia/gridd.conf regexp='^FeatureType=0' line='FeatureType=2'

    - name: Add NVIDIA GRID License GUI
      lineinfile: dest=/etc/nvidia/gridd.conf regexp='^#EnableUI=TRUE' line='EnableUI=TRUE'

    - name: Stop nvidia-gridd service
      service:
        name: nvidia-gridd
        state: stopped

    - name: Remove NVIDIA driver installer file
      file:
        path: /usr/local/NVIDIA-Linux-grid.run
        state: absent
      ignore_errors: yes

    - name: Reboot the VMs to enable the NVIDIA GRID driver
      reboot:
        msg: System reboot inititated by Ansible
        reboot_timeout: 120

    - name: Make sure the is no older version of Horizon on the VMs
      shell: rm -r /home/setup/VMware-*-linux*
      ignore_errors: yes

    - name: Unarchive the Horizon ViewAgent Tarball
      unarchive:
        src: /media/penelopeprime/.Admin/Horizon-7.13/VMware-horizonagent-linux-x86_64-7.13.1-18025243.tar.gz
        dest: /home/setup/
        remote_src: True
        owner: setup
        mode: 0777

    - name: Install and register the Horizon ViewAgent with the Connection server
      shell: cd /home/setup/VMware-*-linux* && ./install_viewagent.sh -r no -S no -A yes -n "{{inventory_hostname}}" -b "{{ brokerip }}" -d "{{ vmdomain }}" -u "{{ brokeruser }}" -p "{{ brokerpass }}"
      register: HorizonInstallLog

    - name: Collecting installation logs
      debug: var=HorizonInstallLog.stdout_lines

    - name: Changing the default network in the Viewagenet configuration file
      lineinfile: dest=/etc/vmware/viewagent-custom.conf regexp='^#Subnet=192.168.1.0/24' line='Subnet=10.33.36.0/22'

    - name: Disabling Keyboard Layout Syncronisation
      lineinfile: dest=/etc/vmware/viewagent-custom.conf regexp='^#KeyboardLayoutSync=FALSE' line='KeyboardLayoutSync=TRUE'

    - name: Enable bi-directional Clipboard option between client and agent
      lineinfile: dest=/etc/vmware/config regexp='^#Clipboard.Direction=1' line='Clipboard.Direction=1'

    - name: Reboot the VMs to apply the Horizon Agent settings
      reboot:
        msg: System reboot inititated by Ansible
        reboot_timeout: 120
      ignore_errors: yes
        

    - name: Install Ubuntu Repo based tools
      apt:
        name: "{{ packages }}"
        update_cache: yes
      vars:
        packages:
        - autoconf
        - libtool
